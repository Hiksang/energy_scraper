import os
import requests
from bs4 import BeautifulSoup
import datetime import datetime
from dotenv import load_dotenv
import logging

from energy_scraper.logger import setup_logger

setup_logger()

# .env íŒŒì¼ ë¡œë“œ (NAS ê²½ë¡œ ë“± ì„¤ì •)
load_dotenv()

BASE_URL = "https://finance.naver.com/research/industry_list.naver"
NAS_PATH = os.getenv("NAS_PATH", "./downloads")  # NAS ê²½ë¡œ (ê¸°ë³¸ê°’: ë¡œì»¬)

def get_research_papers(page: int = 1):
    """ë„¤ì´ë²„ ì¦ê¶Œ ë¦¬ì„œì¹˜ í˜ì´ì§€ì—ì„œ ì—ë„ˆì§€ ì—…ì¢… ë¬¸ì„œ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°"""
    url = f"{BASE_URL}?keyword=&brokerCode=&writeFromDate=&writeToDate=&searchType=upjong&upjong=%BF%A1%B3%CA%C1%F6&page={page}"
    
    response = requests.get(url)
    response.raise_for_status()
    print(f"âœ… ìš”ì²­ ì„±ê³µ: {url}")

    soup = BeautifulSoup(response.text, "html.parser")
    rows = soup.select("table.type_1 tr")  # ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
    print(f"ğŸ” í…Œì´ë¸” í–‰ ìˆ˜: {len(rows)}")  # ì—¬ê¸°ê°€ 0ì´ë©´ ì„ íƒì ë¬¸ì œ

    papers = []
    for row in rows[1:]:
        cols = row.find_all("td")
        if len(cols) < 5:
            continue

        title = cols[1].text.strip()
        company = cols[2].text.strip()
        date = cols[3].text.strip()
        pdf_tag = cols[3].find("a")
        if not pdf_tag or "href" not in pdf_tag.attrs:
            print(f"âŒ PDF ë§í¬ ì—†ìŒ: {title}")
            continue

        pdf_url = pdf_tag["href"]
        print(f"ğŸ“ PDF ìˆìŒ â†’ {title} â†’ {pdf_url}")

        papers.append({
            "title": title,
            "company": company,
            "date": date,
            "pdf_url": pdf_url,
        })

    return papers

def get_all_papers():
    """ë„¤ì´ë²„ ë¦¬ì„œì¹˜ í˜ì´ì§€ì—ì„œ ë§ˆì§€ë§‰ê¹Œì§€ ëŒë©° ëª¨ë“  ë¦¬í¬íŠ¸ ìˆ˜ì§‘"""
    all_papers = []
    page = 1

    while True:
        print(f"ğŸ“„ {page}í˜ì´ì§€ ì²˜ë¦¬ ì¤‘...")
        papers = get_research_papers(page)
        if not papers:
            print("âŒ ë” ì´ìƒ ë¦¬í¬íŠ¸ ì—†ìŒ. ì¢…ë£Œ.")
            break

        all_papers.extend(papers)

        # ë‹¤ìŒ í˜ì´ì§€ ë²„íŠ¼ì´ ìˆëŠ”ì§€ í™•ì¸
        if not has_next_page(page):
            print("âœ… ë§ˆì§€ë§‰ í˜ì´ì§€ ë„ë‹¬")
            break

        page += 1

    return all_papers

def has_next_page(current_page: int) -> bool:
    """í˜„ì¬ í˜ì´ì§€ì—ì„œ 'ë‹¤ìŒ' ë²„íŠ¼ì´ ìˆëŠ”ì§€ í™•ì¸"""
    url = f"{BASE_URL}?searchType=upjong&upjong=%BF%A1%B3%CA%C1%F6&page={current_page}"
    headers = {"User-Agent": "Mozilla/5.0"}
    response = requests.get(url, headers=headers)
    response.raise_for_status()
    soup = BeautifulSoup(response.text, "html.parser")

    next_button = soup.select_one("td.pgR a")
    return next_button is not None



def download_pdf(title: str, pdf_url: str):
    """PDF ë§í¬ë¥¼ ì§ì ‘ ë‹¤ìš´ë¡œë“œí•˜ëŠ” ê°„ë‹¨í•œ ë²„ì „"""
    print(f"ğŸ“¥ ë‹¤ìš´ë¡œë“œ ì¤‘: {title} â†’ {pdf_url}")

    headers = {
        "User-Agent": "Mozilla/5.0"
    }

    response = requests.get(pdf_url, headers=headers)
    response.raise_for_status()

    safe_title = title.replace("/", "_").replace(" ", "_").replace(":", "_").replace("?", "")
    pdf_path = os.path.join(NAS_PATH, f"{safe_title}.pdf")

    with open(pdf_path, "wb") as f:
        f.write(response.content)

    print(f"âœ… ì €ì¥ ì™„ë£Œ: {pdf_path}")


def main():
    os.makedirs(NAS_PATH, exist_ok=True)
    all_papers = get_all_papers()
    print(f"ğŸ“¦ ì´ ë¦¬í¬íŠ¸ ìˆ˜ì§‘ ê°œìˆ˜: {len(all_papers)}")

    for paper in all_papers:
        print(f"ğŸ“¥ ë‹¤ìš´ë¡œë“œ ì¤‘: {paper['title']}")
        download_pdf(paper["title"], paper["pdf_url"])



if __name__ == "__main__":
    main()
